{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pds, numpy as np, tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import nltk, itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensures the dataframe is 50% legit 50% phish:\n",
    "def get5050(df):\n",
    "    nPhish = len(df[df['label']==1])\n",
    "    nLegit = len(df[df['label']==0])\n",
    "    \n",
    "    nMin = min(nPhish, nLegit)\n",
    "    #select the nMin phish-rows\n",
    "    sPhish = df[df['label']==1].sample(n=nMin)\n",
    "    #select the nMin legit-rows\n",
    "    sLegit = df[df['label']==0].sample(n=nMin)\n",
    "    \n",
    "    if(len(sPhish)!=len(sLegit)):\n",
    "        print(\"Error laoding 50/50 dataset\")\n",
    "        \n",
    "    return sLegit.append(sPhish,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "procURL = False\n",
    "def proc_urls(df, splitter):    \n",
    "    if(not isinstance(splitter,nltk.tokenize.regexp.RegexpTokenizer)):\n",
    "        raise TypeError(\"splitter of proc_url must be nltk RegexpTokenizer\")\n",
    "    \n",
    "    if(procURL==False):\n",
    "        return df\n",
    "    \n",
    "    newUrls = []\n",
    "    for url in df[\"url\"]:\n",
    "        url = str(url).lower()\n",
    "\n",
    "        url.encode('utf-8', 'ignore').decode()\n",
    "        urlSplit = [clean.strip() for clean in splitter.tokenize(url) if(clean.strip()!=\"\")]\n",
    "    \n",
    "        newUrls += \" \".join(urlSplit)\n",
    "    df[\"url\"] = newUrls\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, class_names, cTitle='Confusion Matrix'):\n",
    "    \"\"\"\n",
    "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
    "    NOTE: Modified from output of scikit learn!  \n",
    "    EXPECTS: [[tp, fp],[fn,tn]]\n",
    "    \n",
    "    Args:\n",
    "    cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
    "    class_names (array, shape = [n]): String names of the integer classes\n",
    "    \"\"\"\n",
    "  \n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(cTitle)\n",
    "    #plt.colorbar()\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    #Normalize the confusion matrix.\n",
    "    #cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "    # Use white text if squares are dark; otherwise black.\n",
    "    colm = np.array([[\"white\",\"black\"],[\"black\",\"white\"]])\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=colm[i,j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.xlabel('Truth')\n",
    "  \n",
    "    return figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets create our own pipeline for fine tuning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the setup for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5eac2465b843028861ccb19c084b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1473e85c5b6d4514b3b11855ba5aba89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbdd09c85f794d86907b5e38956cb756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer, ElectraTokenizerFast, RobertaTokenizerFast, TFElectraForSequenceClassification, TFBertForSequenceClassification, TFRobertaForSequenceClassification, TFTrainer, TFTrainingArguments\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\" #this is the checkpoint (pretrained model) we are using for the example.\n",
    "tokenizer = BertTokenizer.from_pretrained(checkpoint) #setup the tokenizer. We need to pass it the checkpoint because hf models need a specific mapping of word-ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's create/load a CSV into a dataset object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24310, 60775)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(6078, 15195)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#load the csv:\n",
    "dfTrain = pds.read_csv(\"../../data/compiled/allTrain.csv\", index_col = 0)\n",
    "dfTest = pds.read_csv(\"../../data/compiled/allTest.csv\", index_col = 0)\n",
    "dfLegit = dfTrain[dfTrain[\"label\"]==0]\n",
    "dfPhish = dfTrain[dfTrain[\"label\"]==1]\n",
    "\n",
    "dfTrainFixed = get5050(dfTrain)\n",
    "dfTestFixed = get5050(dfTest)\n",
    "splitter=nltk.RegexpTokenizer(\"\\d+|[a-z]+|\\W\")\n",
    "dfTrainFixed = proc_urls(dfTrainFixed,splitter)\n",
    "dfTestFixed = proc_urls(dfTestFixed,splitter)\n",
    "\n",
    "display((len(dfTrainFixed), len(dfTrain)))\n",
    "display((len(dfTestFixed), len(dfTest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>erobooker.com/nozarashisatoru/usaginokamentanh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>www.elearningmarketplace.co.uk/product-categor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>blijmetjeboekhouding.nl/aanbevelingen/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>www.exotickenya.com/escorts-from/ongata-rongai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>www.naturallyheaventherapy.co.uk/online-store/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24305</th>\n",
       "      <td>1</td>\n",
       "      <td>linktr.ee/PUBGM.UCfree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24306</th>\n",
       "      <td>1</td>\n",
       "      <td>caspianglobalservices.com/fonts/.hennessy/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24307</th>\n",
       "      <td>1</td>\n",
       "      <td>hannetjiefaurie1.creatorlink.net/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24308</th>\n",
       "      <td>1</td>\n",
       "      <td>halifax-security-login.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24309</th>\n",
       "      <td>1</td>\n",
       "      <td>taijishentie.com/js/index.htm?us.battle.net/lo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24310 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                                url\n",
       "0          0  erobooker.com/nozarashisatoru/usaginokamentanh...\n",
       "1          0  www.elearningmarketplace.co.uk/product-categor...\n",
       "2          0             blijmetjeboekhouding.nl/aanbevelingen/\n",
       "3          0  www.exotickenya.com/escorts-from/ongata-rongai...\n",
       "4          0  www.naturallyheaventherapy.co.uk/online-store/...\n",
       "...      ...                                                ...\n",
       "24305      1                             linktr.ee/PUBGM.UCfree\n",
       "24306      1         caspianglobalservices.com/fonts/.hennessy/\n",
       "24307      1                  hannetjiefaurie1.creatorlink.net/\n",
       "24308      1                        halifax-security-login.com/\n",
       "24309      1  taijishentie.com/js/index.htm?us.battle.net/lo...\n",
       "\n",
       "[24310 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrainFixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(dfTrainFixed[\"url\"])\n",
    "y = list(dfTrainFixed[\"label\"])\n",
    "\n",
    "#X_train, X_val, y_train, y_val\n",
    "xTR, xVA, yTR, yVA = train_test_split(x,y, shuffle=True, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tEncoding = tokenizer(xTR, truncation=True, padding=True)\n",
    "vEncoding = tokenizer(xVA, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDS = tf.data.Dataset.from_tensor_slices((dict(tEncoding),yTR))\n",
    "valDS = tf.data.Dataset.from_tensor_slices((dict(vEncoding),yVA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Lets do some Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support( \\\n",
    "                           labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c60d8dd5f234dc0b7e4a4442a83fabf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFBertForSequenceClassification: ['distilbert', 'pre_classifier', 'dropout_19']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['bert']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/s/chopin/l/grad/mbanik/.local/lib/python3.8/site-packages/transformers/trainer_tf.py:109: FutureWarning: The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. We recommend using native Keras instead, by calling methods like `fit()` and `predict()` directly on the model object. Detailed examples of the Keras style can be found in our examples at https://github.com/huggingface/transformers/tree/master/examples/tensorflow\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#OUTPUT_DIR=\"/s/fir/e/nobackup/Fresh-Phish/transformer_exps/progs/Trained Models\"\n",
    "OUTPUT_DIR=\"../proj/progs/TrainedModels\"\n",
    "training_args = TFTrainingArguments(output_dir=OUTPUT_DIR,            # output directory\n",
    "            evaluation_strategy=\"steps\",      # evaluation strategy\n",
    "            num_train_epochs=3,               # total number of training epochs\n",
    "            per_device_train_batch_size=16,  # batch size per device during training\n",
    "            per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "            per_gpu_train_batch_size=16,\n",
    "            per_gpu_eval_batch_size=16,\n",
    "            warmup_steps=10000,  # number of warmup steps for learning rate scheduler\n",
    "            weight_decay=0.01,   # strength of weight decay\n",
    "            learning_rate=8.e-5, # Adam learning rate\n",
    "            adam_epsilon=1.e-6)  # Adam epsilon\n",
    "\n",
    "with training_args.strategy.scope():\n",
    "    model = TFBertForSequenceClassification.from_pretrained(\n",
    "            \"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    \n",
    "trainer = TFTrainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=trainDS,               # training dataset\n",
    "    eval_dataset=valDS,                  # evaluation dataset\n",
    "    compute_metrics=compute_metrics      # evaluation metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "2021-11-20 21:12:38.917431: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Found an unshardable source dataset: name: \"TensorSliceDataset/_4\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "input: \"Placeholder/_2\"\n",
      "input: \"Placeholder/_3\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT32\n",
      "      type: DT_INT32\n",
      "      type: DT_INT32\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 512\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 512\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 512\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-Tuning Time: 1620.58\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "trainer.train()\n",
    "stop = time.time()\n",
    "print(\"Fine-Tuning Time: {:.2f}\".format(stop-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "2021-11-20 21:39:39.499819: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Found an unshardable source dataset: name: \"TensorSliceDataset/_4\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "input: \"Placeholder/_2\"\n",
      "input: \"Placeholder/_3\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT32\n",
      "      type: DT_INT32\n",
      "      type: DT_INT32\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 512\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 512\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 512\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method TFTrainer.run_model of <transformers.trainer_tf.TFTrainer object at 0x7fea684d4700>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TFTrainer.run_model of <transformers.trainer_tf.TFTrainer object at 0x7fea684d4700>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Accuracy: 0.9730674342105263\n"
     ]
    }
   ],
   "source": [
    "model_result = trainer.evaluate()\n",
    "print(\"Accuracy: {}\".format(model_result['eval_accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.07906411196056165,\n",
       " 'eval_accuracy': 0.9730674342105263,\n",
       " 'eval_f1': 0.9724384599200505,\n",
       " 'eval_precision': 0.9788225328250741,\n",
       " 'eval_recall': 0.9661371237458194}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Looks good. Lets predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTE = list(dfTestFixed[\"url\"])\n",
    "yTE = list(dfTestFixed[\"label\"])\n",
    "#create the test dataset:\n",
    "\n",
    "testEncoding = tokenizer(xTE,truncation=True, padding=True)\n",
    "testDS=tf.data.Dataset.from_tensor_slices((dict(testEncoding), yTE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n",
      "2021-11-20 22:00:25.539418: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Found an unshardable source dataset: name: \"TensorSliceDataset/_4\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "input: \"Placeholder/_1\"\n",
      "input: \"Placeholder/_2\"\n",
      "input: \"Placeholder/_3\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT32\n",
      "      type: DT_INT32\n",
      "      type: DT_INT32\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 512\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 512\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 512\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future version. Using `--per_device_eval_batch_size` is preferred.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Bal Accuracy: 95.986%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_train = trainer.predict(testDS)\n",
    "trlabels = pred_train.label_ids\n",
    "trpreds = pred_train.predictions.argmax(-1)\n",
    "\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(trlabels, trpreds).ravel()\n",
    "traccuracy = (tp + tn)/(tn + tp + fn + fp)\n",
    "traccuracyB = ((tp/(tp+fn) + tn/(fp+tn))*0.5)\n",
    "print(\"Training Bal Accuracy: {:.3f}%\".format(traccuracyB*100.))\n",
    "cmatrix = np.array([[tp, fp],[fn, tn]])\n",
    "disp = plot_confusion_matrix(cmatrix, ['Phish','Legit'], cTitle='Training Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.74540119, 0.        ])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([10 * np.random.uniform(), 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
